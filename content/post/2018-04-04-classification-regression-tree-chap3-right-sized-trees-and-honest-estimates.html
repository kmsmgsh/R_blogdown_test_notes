---
title: Classification & Regression Tree Chap3 Right sized trees and honest estimates
author: Jiaming
date: '2018-04-04'
slug: classification-regression-tree-chap3-right-sized-trees-and-honest-estimates
categories:
  - Statistics
  - statitics notes
tags:
  - decision tree
  - Multivariable analysis
  - statistics, linear model, regression, covariance
---



<div id="copy-from-leo-breiman-jerome-h.-friedman-richard-a.olshen-charles-j.stone-classification-and-regression-trees." class="section level6">
<h6>Copy from Leo Breiman, Jerome H. Friedman, Richard A.Olshen, Charles J.Stone “Classification and regression trees”.</h6>
</div>
<div id="notes-for-chap3-right-sized-trees-and-honest-estimates." class="section level5">
<h5>Notes for Chap3 Right sized trees and honest estimates.</h5>
</div>
<div id="introduction" class="section level4">
<h4>3.1 INTRODUCTION</h4>
<p>Main concerntration:</p>
<ul>
<li>1.Getting the right sized tree T</li>
<li>2.Getting more accurate estimates of the true probability of misclassification or of the true expected missclassification cost <span class="math inline">\(R^*(T)\)</span>
If only use resubstitute data into the constructed tree to estimate the <span class="math inline">\(R(T)\)</span>,it would be downward. (Because use the data to constuct the tree and use the same data to evaluate tree.)</li>
</ul>
<p>If each terminal node only contains one data case(Like 1-NN), then resubstitution estimate of <span class="math inline">\(R(T)\)</span> is 0, but true value is ofc not.
Then the validated tree which is choosen by this underestimated <span class="math inline">\(R(T)\)</span> can hardly with predictive power when tested in another samples dran from the same population distribution.
p.s. More familiar notation is <span class="math inline">\(err(\tau)\)</span>, the training error, is same meaning with the <span class="math inline">\(R(T)\)</span>.</p>
<p>Too large a tree will have a higher true misclassification rate than the right sized tree.
Too small a tree will not use some of the classification information available in <span class="math inline">\(T\)</span>, again resulting in a higher true misclassification rate than the right sized tree.</p>
<p>Showed in talbe that
1. Training error always decrease when terminal nodes increasing.
2. <span class="math inline">\(R^{TS}\)</span> increase initially decreasing in size when size of tree decrease and hit the minimum in 10, than begins to climb again when the trees get too small.</p>
<p>So the key point is which the splits were “informative”. That means, finding appropriate stopping rules.</p>
<p>Recall one of the early stopping rule is about the impurity
<span class="math display">\[
\operatorname*{\max}_s \triangle I(s,t)&lt;\beta
\]</span>
If the <span class="math inline">\(\beta\)</span> is set too low, then there is too much splitting and the tree is too large. Increasing <span class="math inline">\(\beta\)</span> to some threshold also leads to some difficulties. It’s hard to find a proper threshold. There may be node t is not that good split, but the split in t to <span class="math inline">\(t_L,t_R\)</span> may have splits with large decreases in impurity.
Finally, looking for a more satisfactory procedure was found that consisted of two key elements:</p>
<ol style="list-style-type: decimal">
<li><p>Prune instead of stopping. Grow a tree that is much too large and prune it upward in the “right way” until you finally cut back to the root node. (This can avoid the situation mentioned before, just not cut the split with significant decreasing)</p></li>
<li><p>Use more accurate estimates of <span class="math inline">\(R^*(T)\)</span> to select the right sized tree from among the pruned subtrees.</p></li>
</ol>
<p>Two immediate questions rised in this new framework.</p>
<ul>
<li>How does one prune upward in the “right way”</li>
<li>How can better estimates of <span class="math inline">\(R^*(T)\)</span> be gotten.</li>
</ul>
<p>These two questions will be discussed in 3.2,3.3 ad 3.4.</p>
</div>
<div id="getting-ready-to-prune" class="section level4">
<h4>3.2 GETTING READY TO PRUNE</h4>
<p><span class="math display">\[
\begin{align}
R(T)&amp;=\sum_{t\in \tilde{T}}r(t)p(t)\\
&amp;=\sum_{t\in \tilde{T}} R(t).
\end{align}
\]</span>
Refer <span class="math inline">\(R(T)\)</span> and <span class="math inline">\(R(t)\)</span> as the tree and node misclassification costs.</p>
</div>
