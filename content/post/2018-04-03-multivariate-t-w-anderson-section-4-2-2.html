---
title: Multivariate，T.W. Anderson, Section 4.2.2
author: Jiaming
date: '2018-04-03'
slug: multivariate-t-w-anderson-section-4-2-2
categories:
  - Mathematics
  - multivariable
  - Statistics
  - statitics notes
tags:
  - mathematics
  - Multivariable analysis
  - statistics, linear model, regression, covariance
---



<p>Copy the book.
e-version(rather than pen version—&gt; can’t be used by search tools (ctrl+f)).</p>
<div id="goal-find-the-distribution-of-the-sample-correlation-coefficient-when-the-population-coefficient-is-different-from-zero.the-case-is-zero-discussed-in-the-previous-chapter-and-derive-the-distribution-construct-the-hypothesis-test-based-on-the-distribution" class="section level5">
<h5>goal: find the distribution of the sample correlation coefficient when the population coefficient is different from zero.(The case is zero discussed in the previous chapter, and derive the distribution, construct the hypothesis test based on the distribution)</h5>
<p>First thing we should do is derive the joint density of <span class="math inline">\(a_{11},a_{12},a_{22}\)</span>.
In previous section, we saw that conditional on <span class="math inline">\(v_1\)</span> held fixed, the random variable <span class="math inline">\(b=\frac{a_{12}}{a_{11}}\)</span> and <span class="math inline">\(U/\sigma^2=(a_{22}-a^2_{12}/a_{11})/\sigma^2\)</span> are distributed independently according to <span class="math inline">\(N(\beta,\sigma^2/c^2)\)</span> and the <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n-1\)</span> degrees of freedom, respectively.</p>
<p>Remember the previous section. The correlation coefficients are the cosine of the angle by two variable <span class="math inline">\(v_2\)</span> and <span class="math inline">\(bv_1\)</span>. What is <span class="math inline">\(v_1,v_2\)</span>? This would back to the definition of the covariance coeeficient <span class="math inline">\(a_{ij}\)</span>.</p>
<p>Definition: Sample correlation coefficient <span class="math inline">\(a_{ij}\)</span>:
<span class="math display">\[
a_{ij}=\sum ^N _{\alpha=1} (x_{i\alpha}-\overline x_i)(x_{j\alpha}-\overline x_j)
\]</span>
This can be demonstrated by
<span class="math display">\[
a_{ij}=\sum ^n _{\alpha=1} z_{i\alpha}z_{j\alpha}
\]</span>
where n=N-1,
<span class="math display">\[
\begin{eqnarray*}
\begin{pmatrix}
z_{1\alpha}\\
z_{2\alpha}\\
\end{pmatrix} &amp; \sim &amp; N\left[\left(\begin{array}{c}
0\\
0\\
\end{array}\right),\left(\begin{array}{cc}
\sigma_1^2 &amp; \sigma_1\sigma_2\rho \\
\sigma_2\sigma_1\rho &amp; \sigma_2^2 \\
\end{array}\right)\right]\\
\end{eqnarray*}
\]</span>
<span class="math inline">\(z\)</span> is orthogonal linear transform of the <span class="math inline">\(X\)</span>, which is <span class="math inline">\(Z=BX\)</span>, B is an <span class="math inline">\(N\times N\)</span> orhogonal matrix with the last row equals to <span class="math inline">\((1/\sqrt{(N)},....,1/\sqrt{(N)})\)</span>. This transformation can make Z is nearly same as X but the last row <span class="math inline">\(Z_N=\sqrt{N}\overline X\)</span>, which makes the matrix A follows that
<span class="math display">\[
A=\sum _{\alpha=1} ^N X_\alpha X_{\alpha}&#39;-N\overline X\overline X&#39;= \sum _{\alpha=1} ^N Z_\alpha Z_{\alpha}&#39;-Z_NZ&#39;_N=\sum _{\alpha=1} ^{N-1} Z_\alpha Z_{\alpha}&#39;
\]</span>
The form of A looks like the sample covariance matrix?
Here is my attempt
<span class="math display">\[
\sum _\alpha (X_\alpha-\overline X)&#39;(X_\alpha-\overline X)\\
=\sum_\alpha (X_\alpha&#39;X_\alpha-X&#39;_\alpha\overline X-\overline X&#39;X_\alpha+\overline X&#39;\overline X)\\
=\sum_\alpha X_\alpha&#39;X_\alpha-N\overline X&#39;\overline X\\
=A
\]</span>
So the transform makes the formula simple like the minus <span class="math inline">\(\overline x\)</span> term is not exist, just two <span class="math inline">\(z\)</span> times together.</p>
<p>Okay, back the the <span class="math inline">\(v\)</span>,<span class="math inline">\(v_i=(z_{i1},...,z_{in})&#39;,i=1,2.\)</span> Then, let <span class="math inline">\(b=v&#39;_2v_1/v_1&#39;v_1\)</span>, then <span class="math inline">\(v_2-bv_1\)</span> is orthogonal to <span class="math inline">\(v_1\)</span> and then we can got the information of the angle <span class="math inline">\(\theta\)</span> with <span class="math inline">\(cot\ \theta=\frac{b||v_1||}{||v_2-bv_1||}\)</span>. After b, then it’s <span class="math inline">\(U\)</span>.
Original <span class="math inline">\(U\)</span> is
<span class="math display">\[
U=(V_2-bv_1)&#39;(V_2-bv_1)=V_2&#39;V_2-b^2v&#39;_1v_1=a_{22}-a_{12}^2/a_{11}\\ =\sum _{\alpha=1}^n Z_{2\alpha}^2-b^2\sum_{\alpha=1}^n z^2_{1\alpha}=\sum_{\alpha=1}^nY_\alpha^2-Y_1^2=\sum_{\alpha}^n Y_\alpha^2 
\]</span>
Which is independent of b. U is constructed by the definition of <span class="math inline">\(cot\ \theta\)</span>, the denominator <span class="math inline">\(||v_2-bv_1||\)</span>. Then the <span class="math inline">\(cot\ \theta=b\sqrt{a_{11}/U}\)</span>. The construction of Y is similar to <span class="math inline">\(Z\)</span>, to make the formula simpler. Then Y followed normal distribution, then the sum square of normal distribution is <span class="math inline">\(\chi^2\)</span> distribution. So get the result that <span class="math inline">\(U/\sigma^2\)</span> has a <span class="math inline">\(\chi^2\)</span> distribution with df is <span class="math inline">\(n-1\)</span>.</p>
<p>Then we finish describing the things provided by previous section. Back to our original points.
The distribution of <span class="math inline">\(b\)</span> and <span class="math inline">\(U/\sigma^2\)</span>.
Denoting the density of the <span class="math inline">\(\chi^2\)</span> by <span class="math inline">\(g_{n-1}(u)\)</span>, then we write the conditional density of b and U as <span class="math inline">\(n(b|\beta,\sigma^2/a_{11})g_{n-1}(u/\sigma^2)/\sigma^2\)</span>. The joint density of <span class="math inline">\(V_1,b,U\)</span> is <span class="math inline">\(n(v_1|0,\sigma_1^2I)n(b|\beta,\sigma^2/a_{11})g_{n-1}(u/\sigma^2)/\sigma^2\)</span>. The marginal density of <span class="math inline">\(V_1&#39;V_1/\sigma_1^2=a_{11}/\sigma_1^2\)</span> is <span class="math inline">\(g_n(u)\)</span>; that is the density of <span class="math inline">\(a_{11}\)</span> is
<span class="math display">\[
\frac{1}{\sigma^2_1} g_n(\frac{a_{11}}{\sigma^2_1})=\operatorname*{\int\dots\int}_{v_1&#39;v_1=a_{11}} n(v_1|0,\sigma_1^2I)dW
\]</span>
where dW is the proper volume element.</p>
<p>Then after tons of proof that can’t understand, it derive the distribution (from the joint distribution)
Theorem 4.2.2 The correlation coefficient in a sample of N from a bivariate normal distribution with correlation <span class="math inline">\(\rho\)</span> is distributed with density
<span class="math display">\[\frac{2^{n-2}(1-\rho^2)^{\frac{1}{2}n}(1-r^2)^{\frac{1}{2}(n-3)}}{(n-2)!\pi}\sum_{\alpha=0}^\infty \frac{(2\rho r)^{\alpha}}{\alpha!}\Gamma^2[\frac{1}{2}(n+\alpha)]\]</span>
where <span class="math inline">\(n=N-1\)</span></p>
<p>Back to the can’t understand proof to find some useful information.</p>
<p>First is r. Found in page 124. This is the guidd light in the maze of proof.
“We want to find the density of”
<span class="math display">\[
r=\frac{a_{12}}{\sqrt{a_{11}a_{22}}}=\frac{a_{12}/(\sigma_1\sigma_2)}{\sqrt{(a_{11}/\sigma_1^2)(a_{22}/\sigma_2^2)}}=\frac{a^*_{12}}{\sqrt{a^*_{11}a_{22}^*}}
\]</span>
Then the density of r give in the theorem.</p>
<p>つつく</p>
</div>
