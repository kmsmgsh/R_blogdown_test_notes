---
title: LDA some reviews
author: Jiaming Shen
date: '2018-02-27'
slug: lda-some-reviews
categories: statitics notes
tags: statistics, linear model, regression, covariance
---



<div id="random-intercept-model" class="section level3">
<h3>Random Intercept Model:</h3>
<p><span class="math display">\[
\begin{align}
&amp;Y_{ij}=\mu_{ij}+b_i+\epsilon_{ij}\\
&amp;E_{ij}=\mu_{ij},b_i\sim N(0,\sigma_b^2),\epsilon_{ij}=N(0,\sigma^2_\epsilon)
\end{align}
\]</span>
The matrix form is
<span class="math display">\[
Y_i=\mu_i+1_{n_i}b_i+\epsilon_i
\]</span></p>
<p>Under this setting, the variance of <span class="math inline">\(Y_i\)</span> is</p>
<p><span class="math display">\[
\begin{align}
Var(Y_i)&amp;=Var(1_{n_i}b_i+\epsilon_i)\\
&amp;=Var(1_{n_i}b_i)+Var(\epsilon_i)\\
&amp;=\sigma_b^2J_{n_i}+\sigma_\epsilon^2I_{n_i}\\
&amp;=(\sigma_b^2+\sigma^2_\epsilon)[(1-\frac{\sigma^2_b}{\sigma_b^2+\sigma^2_\epsilon})I_{n_i}+\frac{\sigma^2_b}{\sigma_b^2+\sigma^2_\epsilon}J_{n_i}]\\
&amp;=\sigma^2[(1-\rho)I_{n_i}+\rho J_{n_i}]
\end{align}
\]</span>
Which is followed the compound symmetry(Uniform/Exchangeable correlation.)
<span class="math display">\[
\Sigma_i=\sigma^2\begin{bmatrix}
    1 &amp; \rho  &amp; \dots  &amp; \rho \\
    \rho &amp; 1  &amp; \dots  &amp; \rho \\
    \vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\
    \rho &amp; \rho &amp; \dots  &amp; 1
\end{bmatrix}_{n_i\times n_i}
\]</span></p>
</div>
<div id="exponential-correlation-structure" class="section level3">
<h3>Exponential correlation structure</h3>
<p><span class="math display">\[
\sigma_{ijk}=\sigma^2exp(-\phi|t_{ij}-t_{ik}|)
\]</span>
If <span class="math inline">\(t_{ij}\)</span> are equally spaced, which is <span class="math inline">\(t_{ij}-t_{i,j+1}=d\)</span>, then the above structure can be simplify to
<span class="math display">\[
\sigma_{ijk}=\sigma^2 \rho^{|j-k|}
\]</span>
This is autoregressive correlation with order one, that is, AR(1) covariance/correlation structure.
<span class="math display">\[
\Sigma_i=\sigma^2\begin{bmatrix}
    1 &amp; \rho  &amp; \dots  &amp; \rho^{n_i-1} \\
    \rho &amp; 1  &amp; \dots  &amp; \rho^{n_i-2} \\
    \vdots &amp; \vdots  &amp; \ddots &amp; \vdots \\
    \rho^{n_i-1} &amp; \rho^{n_i-2}  &amp; \dots  &amp; 1
\end{bmatrix}_{n_i\times n_i}
\]</span>
Justification of AR(1) structure:
<span class="math display">\[
\begin{align}
Y_{ij}&amp;=\mu_{ij}+W_{ij}\\
W_{ij}&amp;=\rho W_{i(j-1)}+\epsilon_{ij}\\
W_{ij}&amp;\bot \epsilon_{ij},\epsilon_{ij}\sim N(0,\sigma^2(1-\rho^2))\ with\ |\rho|&lt;1
\end{align}
\]</span>
<span class="math inline">\(W_{ij}\)</span> are random variable with <span class="math inline">\(Var(W_{ij})=\sigma^2\)</span>
<span class="math display">\[
\begin{align}
Cov(Y_{i(j-1)},Y_{ij})&amp;=Cov(W_{i(j-1)},W_{ij})\\
&amp;=Cov(W_{i(j-1)},\rho W_{i(j-1)})+Cov(W_{i(j-1)},\epsilon_{ij})\\
&amp;=\rho Var(W_{i(j-1)})=\sigma^2\rho
\end{align}
\]</span></p>
</div>
<div id="autoregressive-moving-average-arma-structure" class="section level3">
<h3>Autoregressive moving average (ARMA) structure:</h3>
<p>ARMA(1,1): AR(1)+compound symmetry model.
lag 1 correlation is
<span class="math display">\[
Corr(Y_{ij},Y_{i(j-1)})=\gamma
\]</span>
<span class="math inline">\(\rho\)</span> is additional in correlation in each lag.
<span class="math display">\[
Corr(Y_{ij},Y_{i,j-k})=\gamma \rho^{k-1}
\]</span>
<span class="math display">\[
\Sigma_i=\sigma^2\begin{bmatrix}
    1 &amp; \gamma  &amp;\gamma \rho&amp; \dots  &amp; \gamma\rho^{n_i-2} \\
    \gamma &amp; 1  &amp;\gamma &amp;\dots  &amp; \gamma\rho^{n_i-3} \\
    \vdots &amp; \vdots&amp;\vdots  &amp; \ddots &amp; \vdots \\
    \gamma\rho^{n_i-2} &amp; \gamma\rho^{n_i-3}&amp; \gamma\rho^{n_i-4} &amp; \dots  &amp; 1
\end{bmatrix}_{n_i\times n_i}
\]</span>
- <span class="math inline">\(\rho=1\)</span> is compound symmetry model
- <span class="math inline">\(\gamma=\rho\)</span> is autoregressive model
- <span class="math inline">\(\rho=0\)</span> is the moving average or MA model.</p>
<p>The order-1 moving average
<span class="math display">\[
\Sigma_i=\sigma^2\begin{bmatrix}
    1 &amp; \gamma  &amp;0&amp; \dots  &amp;0 \\
    \gamma &amp; 1  &amp;\gamma &amp;\dots  &amp; 0 \\
    \vdots &amp; \vdots&amp;\vdots  &amp; \ddots &amp; \vdots \\
    0 &amp; 0&amp;0  &amp; \dots  &amp; 1
\end{bmatrix}_{n_i\times n_i}
\]</span>
presenting the moving average model is
<span class="math display">\[
Y_{ij}=\beta\epsilon_{i(j-1)}+\epsilon _{ij},\ Var(\epsilon_{ij})=\tau^2
\]</span>
Then we have
<span class="math display">\[
Var(Y_{ij})=\tau^2(1+\beta^2)
\]</span>
in the <span class="math inline">\(\Sigma_i\)</span> we have
<span class="math display">\[
\gamma=\frac{\beta}{1+\beta^2}, \sigma^2=\tau^2(1+\beta^2)
\]</span>
Then for more general case, ARMA(1,1) have the representing as
<span class="math display">\[
Y_{ij}=\mu_{ij}+\rho W_{i(j-1)}+\beta \epsilon_{i(j-1)}+\epsilon_{ij}
\]</span>
For more general cases, the ARMA(p,q) model can be expressed by
<span class="math display">\[
W_{ij}=\sum^p_{r=1}\rho_r W_{i(j-r)}+\sum_{k=1}^q \beta_k\epsilon_{i(j-k)}+\epsilon_{ij}
\]</span></p>
<div id="toeplitz-or-banded-covariance-structure" class="section level4">
<h4>Toeplitz or Banded covariance structure</h4>
<p><span class="math display">\[
\Sigma_i=\sigma^2\begin{bmatrix}
    1 &amp; \rho_1  &amp;\rho_2&amp; \rho_3  &amp;\rho_4\\
    \rho_1 &amp; 1  &amp;\rho_1 &amp;\rho_2  &amp; \rho_3 \\
    \rho_2 &amp; \rho_1&amp;1  &amp;\rho_1 &amp; \rho_2 \\
    \rho_3 &amp;\rho_2&amp;\rho_1  &amp; 1  &amp; \rho_2\\
    \rho_4&amp;\rho_3&amp;\rho_2&amp;\rho_1&amp;1
\end{bmatrix}_{n_i\times n_i}
\]</span></p>
<p>The correlation stucture is that the same lag share the same covariance.
AR(1) and ARMA(1,1) covariance structures are special cases of the banded covariances.</p>
</div>
<div id="generalunstructured-covariances" class="section level4">
<h4>General/Unstructured covariances:</h4>
<p><span class="math inline">\(\Sigma\)</span> is totally unknown, then the number of unknown parameters is <span class="math inline">\(\sum _{i=1}^n \frac{n_i(n_i+1)}{2}\)</span>. The problem here is that there are so many parameters but not that many of observations. Then this is a not identifiable model.</p>
<p><span class="math inline">\(\Sigma_i=\sigma^2V_i\)</span></p>
<p>….tbc</p>
</div>
<div id="restricted-maximum-likelihood-estimation" class="section level4">
<h4>Restricted maximum likelihood estimation</h4>
<p>Normal linear models with correlated random errors,i.e.</p>
<p><span class="math display">\[
Y\sim N_N(X\beta,\sigma^2V),V=diag(V_1,...,V_m)
\]</span>
V depend on a new parameter scalar/vector <span class="math inline">\(\theta\)</span>, that is <span class="math inline">\(V\equiv V(\theta)\)</span>.</p>
<p>Restricted maximum likelihood (REML) estimation procedure is to obtain MLE of variance components <span class="math inline">\(\theta\)</span> in V and <span class="math inline">\(\sigma\)</span> based on a linearly transformed set of data
<span class="math display">\[
Y^*=AY
\]</span>
A need to make the distribution of <span class="math inline">\(Y^*\)</span> does not depend on the fixed effects <span class="math inline">\(\beta\)</span>.
One way is
<span class="math display">\[
A=I_N-X(X&#39;X)^{-1}X&#39;
\]</span>
This converts Y to the OLS residuals, because the origin Y followed the Normal distribution, A is a linear combination.
The rank of matrix A is n-p, here is the proof
<span class="math display">\[
\begin{align}
rank(A)=trace(A)&amp;=trace(I_N-X(X&#39;X)^{-1}X&#39;)\\
&amp;=n-trace(X(X&#39;X)^{-1}X&#39;)\\
&amp;=n-trace(I_p)\\
&amp;=n-p
\end{align}
\]</span>
Then A is a singular matrix and so does <span class="math inline">\(AVA&#39;\)</span>.Some results showed that the resulting estimates of <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(\theta\)</span> is not depend on the choice of A. So any rank N-p matrix A satisfying <span class="math inline">\(E(Y^*)=0\)</span></p>
<p>Then distribution of <span class="math inline">\(Y^*\)</span> follows the Normal distribution with
<span class="math display">\[
\begin{align}
E(Y^*)=AE(Y)=AX\beta=[I_N-X(X&#39;X)^{-1}X&#39;]X\beta=0\\
Var(Y^*)=AVar(Y)A&#39;=\sigma^2AVA&#39;
\end{align}
\]</span>
The distribution of <span class="math inline">\(Y^*\)</span> is not depend on fixed effexts <span class="math inline">\(\beta\)</span></p>
<blockquote>
<p>Comments:
Why need do such thing?
Not really got the idea in the P85/(187)
and the quantity of N-p????? May in the setting inside the likelihood deducing.</p>
</blockquote>
<blockquote>
<p>Answer to Comments:
The origin idea of get the estimation of <span class="math inline">\(\beta\)</span> is get the <span class="math inline">\(\beta(\sigma,V(\theta))\)</span>, first, then use this to get MLE of <span class="math inline">\(\hat \sigma\)</span>, then get the <span class="math inline">\(\hat V(\hat \theta)\)</span>. Finally to substitute back to <span class="math inline">\(\beta(\sigma,V(\theta))\)</span> to get the estimation of fixed effect <span class="math inline">\(\beta\)</span>. Then this idea generalized by escape the step of get the <span class="math inline">\(\hat \beta(V,\sigma)\)</span>, directly get the <span class="math inline">\(V(\hat \theta)\)</span>, then substitute back to get <span class="math inline">\(\theta\)</span>.</p>
</blockquote>
</div>
<div id="the-restricted-likelihood-estimation-reml" class="section level4">
<h4>The restricted likelihood estimation (REML)</h4>
<p>…tbc…</p>
</div>
<div id="the-linear-mixed-model.lmm" class="section level4">
<h4>The linear mixed model.(LMM)</h4>
<p>Linear model+random effects.</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(Y_i\)</span> is <span class="math inline">\(n_i\)</span> dimensionvector of response for the i-th subject (<span class="math inline">\(i=1,2,...,m\)</span>), observations being record at <span class="math inline">\(t_i=(t_{i1},t_{i2},...,t_{in_i})&#39;\)</span> with
<span class="math display">\[
Y_i=\mathbf{Z}_i\beta_i+\epsilon_i\   i=1,2,...m
\]</span>
<span class="math inline">\(\mathbf{Z}_i\)</span> is an (<span class="math inline">\(n_i\times q\)</span>) design matrix of known covariates and <span class="math inline">\(\beta_i\)</span> is an <span class="math inline">\(q\times 1\)</span> vector of subject specific regression parameters. <span class="math inline">\(\epsilon_i\)</span> are assumed to be <span class="math inline">\(N_{n_i}(0,R_i)\)</span>.</p></li>
<li><p>The second stage is to model the heterogeneity between individuals:
<span class="math display">\[
\beta_i=\mathbf{K}_i\beta+\mathbf{b}_i,\ \ \ i=1,2,...,m
\]</span>
where <span class="math inline">\(\mathbf{K}_i\)</span> is an (<span class="math inline">\(q\times p\)</span>) design matix of known covariates and <span class="math inline">\(\beta\)</span> is an (<span class="math inline">\(p\times 1\)</span>) vector of unknown regression parameters.</p></li>
</ol>
<p>The q-dimensional <span class="math inline">\(\mathbf{b}_i\)</span> are random vectors, assumed to be independent and multivariate normally distributed, <span class="math inline">\(\mathbf{b}_i\sim N_q(0,\mathbf{G})\)</span>.</p>
</div>
<div id="linear-mixed-model-or-laird-ware-model" class="section level4">
<h4>Linear mixed model or Laird-Ware model:</h4>
<p><span class="math display">\[
\mathbf{Y}_i=\mathbf{X}_i \beta+\mathbf{Z}_i\mathbf{b}_i+\epsilon_i, (i=1,2,...,m)
\]</span>
where <span class="math inline">\(\mathbf{b}_1,...,\mathbf{b}_m,\epsilon_1,...,\epsilon_m\)</span> are assumed to be independent, <span class="math inline">\(\mathbf{X}_i=\mathbf{Z}_i\mathbf{K}_i\)</span>,<span class="math inline">\(\beta\)</span> is the fixed effects and <span class="math inline">\(\mathbf{b}_i\)</span> the subject specific random effects.
<span class="math display">\[
\mathbf{Y}_i\sim N(\mathbf{X}_i\beta,\mathbf{\Sigma}_i),\mathbf{\Sigma}_i=\mathbf{R}_i+\mathbf{Z}_i\mathbf{G}\mathbf{Z}&#39;_i
\]</span>
Compromise all the observation in <span class="math inline">\(\mathbf{Y}\)</span>, that is <span class="math inline">\(\mathbf{Y}=(\mathbf{Y}_1,...,\mathbf{Y}_m)&#39;\)</span>. Then
<span class="math display">\[
\mathbf{Y}\sim \mathbf{N}_N(\mathbf{X}\beta,\mathbf{\Sigma})
\]</span>
with <span class="math inline">\(N=\sum^m_{i=1} n_i\)</span>,<span class="math inline">\(\mathbf{X}=(\mathbf{X}_1,\dots,\mathbf{X}_m)&#39;\)</span>,<span class="math inline">\(\mathbf{\Sigma}\)</span> with the following form
<span class="math display">\[
\Sigma=\begin{bmatrix}
    \mathbf{\Sigma}_1 &amp;0&amp; \dots  &amp;0 \\
    0 &amp; \mathbf{\Sigma}_2   &amp;\dots  &amp; 0 \\
    \vdots &amp; \vdots&amp; \ddots &amp; \vdots \\
    0 &amp; 0 &amp; \dots  &amp; \mathbf{\Sigma}_m
\end{bmatrix}
\]</span>
The LMM effectively fits a statistical model to repeated measurement for each subject.</p>
</div>
</div>
